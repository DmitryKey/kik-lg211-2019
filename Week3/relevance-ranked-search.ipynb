{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relevance-ranked search\n",
    "\n",
    "Let's return to the indexing of toy data, as we did in the tutorial on Boolean search. This new tutorial has also been inspired by course material by Filip Ginter in Turku.\n",
    "\n",
    "Our documents now look slightly different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"This is a silly silly silly example\",\n",
    "             \"A better example\",\n",
    "             \"Nothing to see here nor here nor here\",\n",
    "             \"This is a great example and a long example too\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can index them as we did before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix:\n",
      "\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [1 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "cv = CountVectorizer(lowercase=True, binary=True)\n",
    "binary_dense_matrix = cv.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"Term-document matrix:\\n\")\n",
    "print(binary_dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll remove the `binary=True` optional argument from the `CountVectorizer` constructor. The default value is `binary=False`. What change can we observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term-document matrix:\n",
      "\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 2]\n",
      " [0 0 0 1]\n",
      " [0 0 3 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [3 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(lowercase=True)\n",
    "dense_matrix = cv.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"Term-document matrix:\\n\")\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run a query on the term \"example\", we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: example\n",
      "[[1 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "t2i = cv.vocabulary_  # shorter notation: t2i = term-to-index\n",
    "print(\"Query: example\")\n",
    "print(dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of seeing *whether* a term occurs in a document, we now see *how many times* the term occurs in each document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example occurs 1 time(s) in document: This is a silly silly silly example\n",
      "Example occurs 1 time(s) in document: A better example\n",
      "Example occurs 0 time(s) in document: Nothing to see here nor here nor here\n",
      "Example occurs 2 time(s) in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "hits_list = np.array(dense_matrix[t2i[\"example\"]])[0]\n",
    "\n",
    "for i, nhits in enumerate(hits_list):\n",
    "    print(\"Example occurs\", nhits, \"time(s) in document:\", documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the number and sizes of the documents grow, we may think that the more times a search term occurs in a document, the more relevant the document is. So, if we search for \"example\" in our toy document collection, the fourth document is most relevant (2 hits), the first and second documents come next (1 hit each) and the third document is irrelevant (0 hits).\n",
    "\n",
    "If we have multiple search terms, we might think that the more times the search terms occur in total in the document, the more relevant the document is.\n",
    "\n",
    "Note that the bit-wise logical operators `AND (&)` and `OR (|)` will not work properly anymore when our matrix contains word counts. The same applies to `NOT (1 - x)`.\n",
    "\n",
    "Let's search for the most relevant document for the query *better example*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: better example\n",
      "Hits of better:         [[0 1 0 0]]\n",
      "Hits of example:        [[1 1 0 2]]\n",
      "Hits of better example: [[1 2 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: better example\")\n",
    "print(\"Hits of better:        \", dense_matrix[t2i[\"better\"]])\n",
    "print(\"Hits of example:       \", dense_matrix[t2i[\"example\"]])\n",
    "print(\"Hits of better example:\", dense_matrix[t2i[\"better\"]] + dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just added the hits together. This means that we did not search for the phrase \"better example\", nor did we search for \"better\" AND \"example\". What we did search for was some kind of \"better\" OR \"example\", in which the sum of the number of occurrences of \"better\" and \"example\" in a document determines the relevance of the document.\n",
    "\n",
    "This means that the second document, which contains one occurrence each of \"better\" and \"example\" is as good a hit as the fourth document, which contains two occurrences of \"example\" and no occurrence of \"better\".\n",
    "\n",
    "Let's do another query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: silly example\n",
      "Hits of silly:         [[3 0 0 0]]\n",
      "Hits of example:       [[1 1 0 2]]\n",
      "Hits of silly example: [[4 1 0 2]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: silly example\")\n",
    "print(\"Hits of silly:        \", dense_matrix[t2i[\"silly\"]])\n",
    "print(\"Hits of example:      \", dense_matrix[t2i[\"example\"]])\n",
    "print(\"Hits of silly example:\", dense_matrix[t2i[\"silly\"]] + dense_matrix[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and also rank (sort) the results by relevance. We leave out the document without a single hit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: [4 1 0 2]\n",
      "List of tuples (nhits, doc_idx) where nhits > 0: [(4, 0), (1, 1), (2, 3)]\n",
      "Ranked (nhits, doc_idx) tuples: [(4, 0), (2, 3), (1, 1)]\n",
      "\n",
      "Matched the following documents, ranked highest relevance first:\n",
      "Score of 'silly example' is 4 in document: This is a silly silly silly example\n",
      "Score of 'silly example' is 2 in document: This is a great example and a long example too\n",
      "Score of 'silly example' is 1 in document: A better example\n"
     ]
    }
   ],
   "source": [
    "hits_list = np.array(dense_matrix[t2i[\"silly\"]] + dense_matrix[t2i[\"example\"]])[0]\n",
    "print(\"Hits:\", hits_list)\n",
    "\n",
    "nhits_and_doc_ids = [ (nhits, i) for i, nhits in enumerate(hits_list) if nhits > 0 ]\n",
    "print(\"List of tuples (nhits, doc_idx) where nhits > 0:\", nhits_and_doc_ids)\n",
    "\n",
    "ranked_nhits_and_doc_ids = sorted(nhits_and_doc_ids, reverse=True)\n",
    "print(\"Ranked (nhits, doc_idx) tuples:\", ranked_nhits_and_doc_ids)\n",
    "\n",
    "print(\"\\nMatched the following documents, ranked highest relevance first:\")\n",
    "for nhits, i in ranked_nhits_and_doc_ids:\n",
    "    print(\"Score of 'silly example' is\", nhits, \"in document:\", documents[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tf-idf\n",
    "\n",
    "As we may guess, pure word counts are not a good indicator of relevance. Frequently occurring words are not usually very interesting from the point of view of information content.\n",
    "\n",
    "One approach to weight terms (words) by their relevance is to use *term frequency / inverse document frequency (tf-idf)* weighting. There is another [tutorial on tf-idf](tf-idf-gutenberg.ipynb) that illustrates how this weighting works.\n",
    "\n",
    "As a matter of fact, scikit-learn library makes it easy for us to compute the tf-idf scores of terms in a document collection. Instead of the class `CountVectorizer` we can use `TfidfVectorizer`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer can be used with many different parameter values. One option is to count ordinary term frequencies. In this setup the resulting matrix should produce the same values as the one produced by the CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer:\n",
      "[[ 0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.]\n",
      " [ 1.  1.  0.  2.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  3.  0.]\n",
      " [ 1.  0.  0.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  2.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 3.  0.  0.  0.]\n",
      " [ 1.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.]\n",
      " [ 0.  0.  0.  1.]]\n",
      "\n",
      "CountVectorizer:\n",
      "[[0 0 0 1]\n",
      " [0 1 0 0]\n",
      " [1 1 0 2]\n",
      " [0 0 0 1]\n",
      " [0 0 3 0]\n",
      " [1 0 0 1]\n",
      " [0 0 0 1]\n",
      " [0 0 2 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 0]\n",
      " [3 0 0 0]\n",
      " [1 0 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Parameters with which TfidfVectorizer does same thing as CountVectorizer\n",
    "tfv1 = TfidfVectorizer(lowercase=True, sublinear_tf=False, use_idf=False, norm=None)\n",
    "tf_matrix1 = tfv1.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer:\")\n",
    "print(tf_matrix1)\n",
    "\n",
    "print(\"\\nCountVectorizer:\")\n",
    "print(dense_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are the same, except that the TfidfVectorizer produces floating-point values, whereas the CountVectorizer produces integer values.\n",
    "\n",
    "Some useful parameters for the TfidfVectorizer are `sublinear_tf`, `use_idf` and `norm`.\n",
    "\n",
    "`sublinear_tf=True` uses logarithmic word frequencies instead of linear ones. That is, if a term occurs 20 times, it is not 20 times more important than a term that occurs once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies):\n",
      "[[ 0.          0.          0.          1.        ]\n",
      " [ 0.          1.          0.          0.        ]\n",
      " [ 1.          1.          0.          1.69314718]\n",
      " [ 0.          0.          0.          1.        ]\n",
      " [ 0.          0.          2.09861229  0.        ]\n",
      " [ 1.          0.          0.          1.        ]\n",
      " [ 0.          0.          0.          1.        ]\n",
      " [ 0.          0.          1.69314718  0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 2.09861229  0.          0.          0.        ]\n",
      " [ 1.          0.          0.          1.        ]\n",
      " [ 0.          0.          1.          0.        ]\n",
      " [ 0.          0.          0.          1.        ]]\n"
     ]
    }
   ],
   "source": [
    "tfv2 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=False, norm=None)\n",
    "tf_matrix2 = tfv2.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies):\")\n",
    "print(tf_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`use_idf=True` factors in the inverse document frequencies. The more documents a term occurs in, the less relevant the term is, in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies and inverse document frequencies):\n",
      "[[ 0.          0.          0.          1.91629073]\n",
      " [ 0.          1.91629073  0.          0.        ]\n",
      " [ 1.22314355  1.22314355  0.          2.07096206]\n",
      " [ 0.          0.          0.          1.91629073]\n",
      " [ 0.          0.          4.02155128  0.        ]\n",
      " [ 1.51082562  0.          0.          1.51082562]\n",
      " [ 0.          0.          0.          1.91629073]\n",
      " [ 0.          0.          3.24456225  0.        ]\n",
      " [ 0.          0.          1.91629073  0.        ]\n",
      " [ 0.          0.          1.91629073  0.        ]\n",
      " [ 4.02155128  0.          0.          0.        ]\n",
      " [ 1.51082562  0.          0.          1.51082562]\n",
      " [ 0.          0.          1.91629073  0.        ]\n",
      " [ 0.          0.          0.          1.91629073]]\n"
     ]
    }
   ],
   "source": [
    "tfv3 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=None)\n",
    "tf_matrix3 = tfv3.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies and inverse document frequencies):\")\n",
    "print(tf_matrix3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If additionally, we use the L2 norm `norm=\"l2\"` we normalize all document vectors (columns) to have a (Euclidian) length of one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TfidfVectorizer (logarithmic term frequencies and inverse document frequencies, normalized document vectors):\n",
      "[[ 0.          0.          0.          0.39494151]\n",
      " [ 0.          0.84292635  0.          0.        ]\n",
      " [ 0.25939836  0.53802897  0.          0.42681878]\n",
      " [ 0.          0.          0.          0.39494151]\n",
      " [ 0.          0.          0.65482842  0.        ]\n",
      " [ 0.32040859  0.          0.          0.31137642]\n",
      " [ 0.          0.          0.          0.39494151]\n",
      " [ 0.          0.          0.52831145  0.        ]\n",
      " [ 0.          0.          0.31202925  0.        ]\n",
      " [ 0.          0.          0.31202925  0.        ]\n",
      " [ 0.85287113  0.          0.          0.        ]\n",
      " [ 0.32040859  0.          0.          0.31137642]\n",
      " [ 0.          0.          0.31202925  0.        ]\n",
      " [ 0.          0.          0.          0.39494151]]\n"
     ]
    }
   ],
   "source": [
    "tfv4 = TfidfVectorizer(lowercase=True, sublinear_tf=True, use_idf=True, norm=\"l2\")\n",
    "tf_matrix4 = tfv4.fit_transform(documents).T.todense()\n",
    "\n",
    "print(\"TfidfVectorizer (logarithmic term frequencies and inverse document frequencies, normalized document vectors):\")\n",
    "print(tf_matrix4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can search the index in the same way as above, even if we use tf-idf weighting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: silly example\n",
      "Hits of silly:         [[ 0.85287113  0.          0.          0.        ]]\n",
      "Hits of example:       [[ 0.25939836  0.53802897  0.          0.42681878]]\n",
      "Hits of silly example: [[ 1.11226949  0.53802897  0.          0.42681878]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Query: silly example\")\n",
    "print(\"Hits of silly:        \", tf_matrix4[t2i[\"silly\"]])\n",
    "print(\"Hits of example:      \", tf_matrix4[t2i[\"example\"]])\n",
    "print(\"Hits of silly example:\", tf_matrix4[t2i[\"silly\"]] + tf_matrix4[t2i[\"example\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and we can rank the documents using the tf-idf scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hits: [ 1.11226949  0.53802897  0.          0.42681878]\n",
      "List of tuples (hits, doc_idx) where hits > 0: [(1.1122694945914164, 0), (0.53802896910335729, 1), (0.42681878177600086, 3)]\n",
      "Ranked (hits, doc_idx) tuples: [(1.1122694945914164, 0), (0.53802896910335729, 1), (0.42681878177600086, 3)]\n",
      "\n",
      "Matched the following documents, ranked highest relevance first:\n",
      "Score of 'silly example' is 1.1123 in document: This is a silly silly silly example\n",
      "Score of 'silly example' is 0.5380 in document: A better example\n",
      "Score of 'silly example' is 0.4268 in document: This is a great example and a long example too\n"
     ]
    }
   ],
   "source": [
    "hits_list4 = np.array(tf_matrix4[t2i[\"silly\"]] + tf_matrix4[t2i[\"example\"]])[0]\n",
    "print(\"Hits:\", hits_list4)\n",
    "\n",
    "hits_and_doc_ids = [ (hits, i) for i, hits in enumerate(hits_list4) if hits > 0 ]\n",
    "print(\"List of tuples (hits, doc_idx) where hits > 0:\", hits_and_doc_ids)\n",
    "\n",
    "ranked_hits_and_doc_ids = sorted(hits_and_doc_ids, reverse=True)\n",
    "print(\"Ranked (hits, doc_idx) tuples:\", ranked_hits_and_doc_ids)\n",
    "\n",
    "print(\"\\nMatched the following documents, ranked highest relevance first:\")\n",
    "for hits, i in ranked_hits_and_doc_ids:\n",
    "    print(\"Score of 'silly example' is {:.4f} in document: {:s}\".format(hits, documents[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It makes sense that the document \"This is a silly silly silly example\" comes up on the top, but why does \"A better example\" now rank higher than \"This is a great example and a long example too\"? The former one contains only one occurrence of \"example\" whereas the latter one contains two. Can you figure out the reason?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
